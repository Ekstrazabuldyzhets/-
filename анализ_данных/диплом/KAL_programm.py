# –ò–º–ø–æ—Ä—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫
import os
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split

import optuna
import time
import json

from sklearn.metrics import mean_squared_error, mean_absolute_error
from optuna.visualization import plot_optimization_history, plot_param_importances

# –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
features_cols = ["Voltage [V]", "Current [A]", "Temperature [degC]", "Power [W]", "Capacity [Ah]"]
target_variable = "SOC [-]"
BATCH_SIZE = 128
EPOCHS = 10
LEARNING_RATE = 0.0001
SEQUENCE_LENGTH = 20

# –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏
# 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º CUDA (NVIDIA GPU)
if torch.cuda.is_available():
    device = torch.device("cuda:0")
# 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º MPS (Apple Silicon GPU)
elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    device = torch.device("mps")
# 3. Fallback –Ω–∞ CPU
else:
    device = torch.device("cpu")


# –ö–ª–∞—Å—Å —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞ –¥–ª—è SOC estimation
class KalmanFilterSOC:
    def __init__(self, process_noise=0.1, measurement_noise=0.1, initial_soc=1.0):
        """
        –§–∏–ª—å—Ç—Ä –ö–∞–ª–º–∞–Ω–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ SOC –±–∞—Ç–∞—Ä–µ–∏
        Parameters:
        - process_noise: —à—É–º –ø—Ä–æ—Ü–µ—Å—Å–∞ (Q)
        - measurement_noise: —à—É–º –∏–∑–º–µ—Ä–µ–Ω–∏–π (R)
        - initial_soc: –Ω–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ SOC
        """
        self.Q = process_noise  # –®—É–º –ø—Ä–æ—Ü–µ—Å—Å–∞
        self.R = measurement_noise  # –®—É–º –∏–∑–º–µ—Ä–µ–Ω–∏–π
        self.soc = initial_soc  # –¢–µ–∫—É—â–∞—è –æ—Ü–µ–Ω–∫–∞ SOC
        self.P = 1.0  # –ö–æ–≤–∞—Ä–∏–∞—Ü–∏—è –æ—à–∏–±–∫–∏ –æ—Ü–µ–Ω–∫–∏
        self.dt = 1.0  # –í—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥ (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º 1 —Å–µ–∫—É–Ω–¥—É)

    def predict(self, current, capacity):
        """
        Prediction step - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ SOC –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏
        """
        # SOC –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–∫–∞ (–∫—É–ª–æ–Ω–æ–≤—Å–∫–∏–π —Å—á–µ—Ç)
        delta_soc = (current * self.dt) / (3600 * capacity)  # dSOC = I*dt/Capacity
        self.soc = self.soc - delta_soc  # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ SOC

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏ –æ—à–∏–±–∫–∏
        self.P += self.Q

        return self.soc

    def update(self, voltage_measurement, predicted_voltage):
        """
        Update step - –∫–æ—Ä—Ä–µ–∫—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–º–µ—Ä–µ–Ω–∏—è –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è
        """
        # Innovation (–æ—à–∏–±–∫–∞ –º–µ–∂–¥—É –∏–∑–º–µ—Ä–µ–Ω–Ω—ã–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ–º)
        y = voltage_measurement - predicted_voltage

        # Innovation covariance
        S = self.P + self.R

        # Kalman gain
        K = self.P / S

        # Update SOC estimate
        self.soc += K * (y / 10.0)  # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ 1V –∏–∑–º–µ–Ω–µ–Ω–∏–µ ‚âà 0.1 SOC –∏–∑–º–µ–Ω–µ–Ω–∏—è

        # Update error covariance
        self.P = (1 - K) * self.P

        return self.soc

    def estimate_soc(self, current, capacity, voltage_measurement, predicted_voltage):
        """
        –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞: prediction + update
        """
        self.predict(current, capacity)
        soc_estimate = self.update(voltage_measurement, predicted_voltage)
        return soc_estimate

# –ö–ª–∞—Å—Å –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞
class KalmanSOCModel:
    def __init__(self, process_noise=0.01, measurement_noise=0.1, capacity=3.0):
        """
        –ú–æ–¥–µ–ª—å SOC –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞

        Parameters:
        - process_noise: —à—É–º –ø—Ä–æ—Ü–µ—Å—Å–∞
        - measurement_noise: —à—É–º –∏–∑–º–µ—Ä–µ–Ω–∏–π
        - capacity: –µ–º–∫–æ—Å—Ç—å –±–∞—Ç–∞—Ä–µ–∏ –≤ Ah
        """
        self.process_noise = process_noise
        self.measurement_noise = measurement_noise
        self.capacity = capacity
        self.kf = None

    def initialize_filter(self, initial_soc=1.0):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞"""
        self.kf = KalmanFilterSOC(
            process_noise=self.process_noise,
            measurement_noise=self.measurement_noise,
            initial_soc=initial_soc
        )

    def predict_voltage(self, current, temperature, soc):
        """
        –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –±–∞—Ç–∞—Ä–µ–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ SOC
        –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é
        """
        # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –ª–∏—Ç–∏–µ–≤–æ–π –±–∞—Ç–∞—Ä–µ–∏
        open_circuit_voltage = 3.0 + 1.2 * soc  # OCV –∫—Ä–∏–≤–∞—è
        internal_resistance = 0.05 + 0.01 * (1 - soc)  # –í–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ
        voltage = open_circuit_voltage - current * internal_resistance

        # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –Ω–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É
        temperature_effect = 0.001 * (temperature - 25)
        voltage += temperature_effect

        return voltage

    def estimate(self, data_sequence):
        """
        –û—Ü–µ–Ω–∫–∞ SOC –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
        """
        if self.kf is None:
            self.initialize_filter()

        soc_estimates = []
        current_soc = 1.0

        for i, row in enumerate(data_sequence):
            current = row[1]  # Current [A]
            voltage = row[0]  # Voltage [V]
            temperature = row[2]  # Temperature [degC]

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ SOC
            predicted_voltage = self.predict_voltage(current, temperature, current_soc)

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ SOC —Å –ø–æ–º–æ—â—å—é —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞
            current_soc = self.kf.estimate_soc(
                current=current,
                capacity=self.capacity,
                voltage_measurement=voltage,
                predicted_voltage=predicted_voltage
            )

            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ SOC –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1]
            current_soc = np.clip(current_soc, 0.0, 1.0)
            soc_estimates.append(current_soc)

        return np.array(soc_estimates)

# Dataset –∫–ª–∞—Å—Å –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–∞–Ω–Ω—ã—Ö
class BatteryDatasetKalman(Dataset):
    def __init__(self, features, targets, source_files, time_steps):
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ numpy array –µ—Å–ª–∏ —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä PyTorch
        self.features = features.cpu().numpy() if torch.is_tensor(features) else features
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –≤ numpy array –µ—Å–ª–∏ —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä PyTorch
        self.targets = targets.cpu().numpy() if torch.is_tensor(targets) else targets
        self.source_files = source_files # —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
        self.time_steps = time_steps # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏

        # —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        self.sequences = self._create_sequences()

    def _create_sequences(self):
        sequences = [] # —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        file_indices = {}

        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
        current_file = self.source_files[0]
        start_idx = 0

        for i in range(1, len(self.source_files)):
            if self.source_files[i] != current_file:
                file_indices[current_file] = (start_idx, i - 1)
                current_file = self.source_files[i]
                start_idx = i
        file_indices[current_file] = (start_idx, len(self.source_files) - 1)

        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
        for file_name, (start, end) in file_indices.items():
            file_length = end - start + 1
            if file_length >= self.sequence_length:
                for i in range(start, end - self.sequence_length + 2):
                    seq_end = min(i + self.sequence_length, end + 1)
                    sequences.append((i, seq_end))

        return sequences

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        start_idx, end_idx = self.sequences[idx]
        features_seq = self.features[start_idx:end_idx]
        targets_seq = self.targets[start_idx:end_idx]

        # –î–ª—è —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞ –Ω–∞–º –Ω—É–∂–Ω–∞ –≤—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        return (torch.FloatTensor(features_seq),
                torch.FloatTensor(targets_seq),
                self.source_files[start_idx:end_idx],
                self.time_steps[start_idx:end_idx])


# —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—â–∏–µ –Ω–∞–ø—Ä—è–º—É—é —Å –¥–∞–Ω–Ω—ã–º–∏
def data_loader_and_standarder(temperatures_directory, directory):
    frames = []
    # 1) –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –¥–∞—Ç–∞—Å–µ—Ç
    for temp_folder in os.listdir(directory):  # –ø—Ä–æ—Ö–æ–¥–∏–º—Å—è –ø–æ –≤—Å–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω—ã–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º
        if temp_folder in temperatures_directory:  # —Ñ—É–Ω–∫—Ü–∏—è —Ñ–∏–ª—å—Ç—Ä—É—é—â–∞—è –∫–∞–∫–∏–µ –∏–º–µ–Ω–Ω–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –±—É–¥—É—Ç –≤–∑—è—Ç—ã
            temp_path = os.path.join(directory, temp_folder)
            for file in os.listdir(temp_path):  # –ø—Ä–æ—Ö–æ–¥–∏–º—Å—è –ø–æ –≤—Å–µ–º —Ñ–∞–π–ª–∞–º –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                if 'Charge' in file or 'Dis' in file:
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –∑–∞—Ä—è–¥–∞/—Ä–∞–∑—Ä—è–¥–∞
                if file.endswith('.csv'):
                    df = pd.read_csv(os.path.join(temp_path, file))  # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ CSV —Ñ–∞–π–ª–∞
                    df['Power [W]'] = df['Voltage [V]'] * df['Current [A]']  # –†–∞—Å—á–µ—Ç –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ú–æ—â–Ω–æ—Å—Ç—å (Power)
                    df['SourceFile'] = file
                    frames.append(df)
    data = pd.concat(frames, ignore_index=True)

    # 2) —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    scaler = StandardScaler()
    data[features_cols] = scaler.fit_transform(data[features_cols])
    return data, scaler

def data_spliter(data, percents):
    # –ò—Ç–æ–≥–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:
    # Train: 80% —Ñ–∞–π–ª–æ–≤
    # Validation: 10% —Ñ–∞–π–ª–æ–≤ (50% –æ—Ç 20%)
    # Test: 10% —Ñ–∞–π–ª–æ–≤ (50% –æ—Ç 20%)
    test_size_for_test, test_size_for_val = percents
    unique_files = np.array(list(set(data['SourceFile'])))
    train_files, temp_files = train_test_split(unique_files, test_size=test_size_for_test, random_state=24)
    val_files, test_files = train_test_split(temp_files, test_size=test_size_for_val, random_state=24)

    train_data = data[data['SourceFile'].isin(train_files)]
    val_data = data[data['SourceFile'].isin(val_files)]
    test_data = data[data['SourceFile'].isin(test_files)]

    return train_data, val_data, test_data

# —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—â–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
def save_hyperparams(hyperparams, file_path):
    with open(file_path, 'w') as f:
        json.dump(hyperparams, f, indent=2)
    print(f"–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {file_path}")

# —á—Ç–µ–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞
def load_hyperparams(file_path):
    # —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
    with open(file_path, 'r') as f:
        hyperparams = json.load(f)
    return hyperparams

# –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –Ω–µ–º
def hyperparams_exist(file_path):
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
    if not os.path.exists(file_path):
        print(f"üìÅ –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return False

    hyperparams = load_hyperparams(file_path)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π
    required = ['process_noise', 'measurement_noise', 'capacity']
    return all(key in hyperparams for key in required)

'''
—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è –æ—Ç –º–æ–¥–µ–ª–∏ –∫ –º–æ–¥–µ–ª–∏ ‚Üë
'''

# –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞
def data_for_kalman_transmuter(train_data, val_data, test_data):
    train_dataset = BatteryDatasetKalman(
        torch.tensor(train_data[features_cols].values, dtype=torch.float32).to(device),
        torch.tensor(train_data[target_variable].values, dtype=torch.float32).to(device),
        train_data['SourceFile'].values,
        train_data['Time [s]'].values
    )

    val_dataset = BatteryDatasetKalman(
        torch.tensor(val_data[features_cols].values, dtype=torch.float32).to(device),
        torch.tensor(val_data[target_variable].values, dtype=torch.float32).to(device),
        val_data['SourceFile'].values,
        val_data['Time [s]'].values
    )

    test_dataset = BatteryDatasetKalman(
        torch.tensor(test_data[features_cols].values, dtype=torch.float32).to(device),
        torch.tensor(test_data[target_variable].values, dtype=torch.float32).to(device),
        test_data['SourceFile'].values,
        test_data['Time [s]'].values
    )

    # —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –±–∞—Ç—á–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)  # Batch size 1 –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

    return train_loader, val_loader, test_loader

# –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞ - –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
def train_kalman_model(model, train_loader, val_loader, epochs, patience=20, min_delta=0.001):
    """
    –û–±—É—á–µ–Ω–∏–µ/–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞
    """
    history = {'train_loss': [], 'val_loss': []}
    best_val_loss = float('inf')
    epochs_no_improve = 0

    for epoch in range(epochs):
        epoch_start_time = time.time()

        # Training phase
        model.train()
        train_loss = 0.0
        train_count = 0

        for sequences, labels, _, _ in train_loader:
            sequences = sequences.squeeze(0).numpy()  # [1, seq_len, features] -> [seq_len, features]
            labels = labels.squeeze(0).numpy()  # [1, seq_len] -> [seq_len]

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∏–ª—å—Ç—Ä —Å –Ω–∞—á–∞–ª—å–Ω—ã–º SOC
            initial_soc = labels[0] if len(labels) > 0 else 1.0
            model.initialize_filter(initial_soc=initial_soc)

            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è SOC
            soc_predictions = model.estimate(sequences)

            # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã)
            min_len = min(len(soc_predictions), len(labels))
            if min_len > 0:
                loss = np.mean((soc_predictions[:min_len] - labels[:min_len]) ** 2)
                train_loss += loss
                train_count += 1

        train_loss = train_loss / train_count if train_count > 0 else float('inf')
        history['train_loss'].append(train_loss)

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_count = 0

        for sequences, labels, _, _ in val_loader:
            sequences = sequences.squeeze(0).numpy()
            labels = labels.squeeze(0).numpy()

            initial_soc = labels[0] if len(labels) > 0 else 1.0
            model.initialize_filter(initial_soc=initial_soc)

            soc_predictions = model.estimate(sequences)

            min_len = min(len(soc_predictions), len(labels))
            if min_len > 0:
                loss = np.mean((soc_predictions[:min_len] - labels[:min_len]) ** 2)
                val_loss += loss
                val_count += 1

        val_loss = val_loss / val_count if val_count > 0 else float('inf')
        history['val_loss'].append(val_loss)

        epoch_end_time = time.time()
        epoch_time = epoch_end_time - epoch_start_time

        # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
        if val_loss < best_val_loss - min_delta:
            best_val_loss = val_loss
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1

        print(f'–≠–ø–æ—Ö–∞ {epoch + 1}/{epochs}, –ü–æ—Ç–µ—Ä—è –æ–±—É—á–µ–Ω–∏—è: {train_loss:.6f}, –ü–æ—Ç–µ—Ä—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {val_loss:.6f}')
        print(f'–í—Ä–µ–º—è, –∑–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –Ω–∞ —ç–ø–æ—Ö—É: {epoch_time:.4f} —Å–µ–∫—É–Ω–¥')

        if epochs_no_improve >= patience:
            print('–°—Ä–∞–±–æ—Ç–∞–ª–∞ –¥–æ—Å—Ä–æ—á–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞')
            break

    return history

# –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞
def hyperparameters_selectioner(train_loader, val_loader):
    # 0) —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä—É—é –º—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º, –¥–∞—é—â–∞—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∑–∞–ø—É—Å–∫–∞—é—â–∞—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ä–∞—Å—á–∏—Ç—ã–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏
    def objective(trial):
        # 0.1) –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞
        process_noise = trial.suggest_float('process_noise', 1e-5, 1e-1, log=True) # —à—É–º –ø—Ä–æ—Ü–µ—Å—Å–∞
        measurement_noise = trial.suggest_float('measurement_noise', 1e-5, 1e-1, log=True) # —à—É–º –∏–∑–º–µ—Ä–µ–Ω–∏–π
        capacity = trial.suggest_float('capacity', 2.5, 3.5)  # –ï–º–∫–æ—Å—Ç—å –±–∞—Ç–∞—Ä–µ–∏ –≤ Ah

        # 0.2) —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        model = KalmanSOCModel(process_noise=process_noise, measurement_noise=measurement_noise, capacity=capacity)

        # 0.3) –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å [–º–æ–¥–µ–ª—å, –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏–µ –¥–∞—Ç—ã, —ç–ø–æ—Ö–∏, device]
        history = train_kalman_model(model, train_loader, val_loader, EPOCHS)

        # 0.4) –ò–∑–≤–ª–µ—á—å –ø–æ—Å–ª–µ–¥–Ω—é—é –ø–æ—Ç–µ—Ä—é –ø—Ä–æ–≤–µ—Ä–∫–∏
        last_val_loss = history['val_loss'][-1] if history['val_loss'] else float('inf')
        return last_val_loss

    # 1) –∑–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç—É–Ω—É –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞, –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=10)

    # 2) –∏–∑–≤–ª–µ–∫–∞–µ–º –ª—É—á—à—É—é –≤–µ—Ä—Å–∏—é
    best_trial = study.best_trial
    print(f"Best trial: {best_trial}")
    best_hyperparams = study.best_trial.params
    print('Best hyperparameters:', best_hyperparams)

    # 3) –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    optimization_history = plot_optimization_history(study)
    optimization_history.show()
    # –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    param_importances = plot_param_importances(study)
    param_importances.show()

    return best_trial, best_hyperparams

# —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ-–∏—Ç–æ–≥–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
def finale_model_trainer(best_hyperparams, train_loader, val_loader, model_path):
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    model = KalmanSOCModel(
        process_noise=best_hyperparams['process_noise'],
        measurement_noise=best_hyperparams['measurement_noise'],
        capacity=best_hyperparams['capacity']
    )

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    history = train_kalman_model(model, train_loader, val_loader, EPOCHS)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    model_data = {
        'hyperparams': best_hyperparams,
        'model_type': 'kalman'
    }
    torch.save(model_data, model_path)
    print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")

# —Ç–µ—Å—Ç–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏
def finale_model_tester(best_hyperparams, test_loader, model_path):
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    model = KalmanSOCModel(
        process_noise=best_hyperparams['process_noise'],
        measurement_noise=best_hyperparams['measurement_noise'],
        capacity=best_hyperparams['capacity']
    )

    test_predictions = []
    test_labels = []

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model.eval()
    for sequences, labels, _, _ in test_loader:
        sequences = sequences.squeeze(0).numpy()
        labels = labels.squeeze(0).numpy()

        initial_soc = labels[0] if len(labels) > 0 else 1.0
        model.initialize_filter(initial_soc=initial_soc)

        soc_predictions = model.estimate(sequences)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç–∫–∏
        test_predictions.extend(soc_predictions)
        test_labels.extend(labels[:len(soc_predictions)])

    # –†–∞—Å—á–µ—Ç –æ—à–∏–±–æ–∫
    test_predictions_np = np.array(test_predictions)
    test_labels_np = np.array(test_labels)

    mse = mean_squared_error(test_labels_np, test_predictions_np)
    mae = mean_absolute_error(test_labels_np, test_predictions_np)

    print(f"Mean Squared Error on Test Set: {mse:.6f}")
    print(f"Mean Absolute Error on Test Set: {mae:.6f}")

    return test_predictions_np, test_labels_np

'''
—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è –æ—Ç –º–æ–¥–µ–ª–∏ –∫ –º–æ–¥–µ–ª–∏ ‚Üì
'''
def main(data_directory_dict, model_path, hyperparams_path):
    # 1) –∑–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ, –∞ —Ç–∞–∫–∂–µ —Å–æ–≤–µ—Ä—à–∞–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É
    directory = data_directory_dict["LG_HG2_processed"]
    temperatures_directory = [folder for folder in os.listdir(directory) if 'degC' in folder]
    data, scaler = data_loader_and_standarder(temperatures_directory, directory)

    # 1.1) —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Ç–µ—Å—Ç–æ–≤—É—é –∏ –æ–±—É—á–∞—é—â–∏–µ –≤—ã–±–æ—Ä–∫–∏
    percents = [0.2, 0.5]
    train_data, val_data, test_data = data_spliter(data, percents)

    # 1.2) –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —á—Ç–µ–Ω–∏—è –∏—Ö –º–æ–¥–µ–ª—å—é
    train_loader, val_loader, test_loader = data_for_kalman_transmuter(train_data, val_data, test_data)

    if not hyperparams_exist(hyperparams_path):
        # 2) –ø–æ–¥–±–∏—Ä–∞–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        print("–ü–æ–¥–±–µ—Ä–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞")
        best_trial, best_hyperparams = hyperparameters_selectioner(train_loader, val_loader)
        # 2.1) —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        save_hyperparams(best_hyperparams, hyperparams_path)
    else:
        # 3) —Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        print("–†–µ–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–≥–Ω–æ–∑ —Å –ø–æ–º–æ—â—å—é —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞")
        best_hyperparams = load_hyperparams(hyperparams_path)
        finale_model_trainer(best_hyperparams, train_loader, val_loader, model_path)
        # 4) —Ç–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        finale_model_tester(best_hyperparams, test_loader, model_path)

    return

if __name__ == "__main__":
    model_path = "/Users/nierra/Desktop/–¥–∏–ø–ª–æ–º-2/–¥–∞—Ç–∞—Å–µ—Ç_2/kalman_soc_model.pth"
    hyperparams_path = "/Users/nierra/Desktop/–¥–∏–ø–ª–æ–º-2/–¥–∞—Ç–∞—Å–µ—Ç_2/kalman_hyperparams.json"
    main_directory = "/Users/nierra/Desktop/–¥–∏–ø–ª–æ–º-2/–¥–∞—Ç–∞—Å–µ—Ç_2/Data"
    data_directory_dict = {"LG_HG2_processed": f"{main_directory}/LG_HG2_processed"}
    main(data_directory_dict, model_path, hyperparams_path)