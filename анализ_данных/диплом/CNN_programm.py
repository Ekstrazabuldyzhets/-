# –ò–º–ø–æ—Ä—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫
import os
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split

import optuna
import time
import json

from sklearn.metrics import mean_squared_error, mean_absolute_error
from optuna.visualization import plot_optimization_history, plot_param_importances

# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏—Ç–µ–∫–∏ —Å –º–æ–¥–µ–ª—å—é
import NN_class as nero

# –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
features_cols = ["Voltage [V]", "Current [A]", "Temperature [degC]", "Power [W]", "Capacity [Ah]"]
target_variable = "SOC [-]"
BATCH_SIZE = 128
EPOCHS = 10
LEARNING_RATE = 0.0001
SEQUENCE_LENGTH = 20

# –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏
# 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º CUDA (NVIDIA GPU)
if torch.cuda.is_available():
    device = torch.device("cuda:0")
# 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º MPS (Apple Silicon GPU)
elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    device = torch.device("mps")
# 3. Fallback –Ω–∞ CPU
else:
    device = torch.device("cpu")

# —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—â–∏–µ –Ω–∞–ø—Ä—è–º—É—é —Å –¥–∞–Ω–Ω—ã–º–∏
def data_loader_and_standarder(temperatures_directory, directory):
    frames = []
    # 1) –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –¥–∞—Ç–∞—Å–µ—Ç
    for temp_folder in os.listdir(directory): # –ø—Ä–æ—Ö–æ–¥–∏–º—Å—è –ø–æ –≤—Å–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω—ã–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º
        if temp_folder in temperatures_directory: # —Ñ—É–Ω–∫—Ü–∏—è —Ñ–∏–ª—å—Ç—Ä—É—é—â–∞—è –∫–∞–∫–∏–µ –∏–º–µ–Ω–Ω–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –±—É–¥—É—Ç –≤–∑—è—Ç—ã
            temp_path = os.path.join(directory, temp_folder)
            for file in os.listdir(temp_path): # –ø—Ä–æ—Ö–æ–¥–∏–º—Å—è –ø–æ –≤—Å–µ–º —Ñ–∞–π–ª–∞–º –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                if 'Charge' in file or 'Dis' in file:
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –∑–∞—Ä—è–¥–∞/—Ä–∞–∑—Ä—è–¥–∞
                if file.endswith('.csv'):
                    df = pd.read_csv(os.path.join(temp_path, file)) # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ CSV —Ñ–∞–π–ª–∞
                    df['Power [W]'] = df['Voltage [V]'] * df['Current [A]'] # –†–∞—Å—á–µ—Ç –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ú–æ—â–Ω–æ—Å—Ç—å (Power)
                    df['SourceFile'] = file
                    frames.append(df)
    data = pd.concat(frames, ignore_index=True)

    # 2) —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    scaler = StandardScaler()
    data[features_cols] = scaler.fit_transform(data[features_cols])
    return data

def data_spliter(data, percents):
    # –ò—Ç–æ–≥–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:
    # Train: 80% —Ñ–∞–π–ª–æ–≤
    # Validation: 10% —Ñ–∞–π–ª–æ–≤ (50% –æ—Ç 20%)
    # Test: 10% —Ñ–∞–π–ª–æ–≤ (50% –æ—Ç 20%)
    test_size_for_test, test_size_for_val = percents
    unique_files = np.array(list(set(data['SourceFile'])))
    train_files, temp_files = train_test_split(unique_files, test_size=test_size_for_test, random_state=24)
    val_files, test_files = train_test_split(temp_files, test_size=test_size_for_val, random_state=24)

    train_data = data[data['SourceFile'].isin(train_files)]
    val_data = data[data['SourceFile'].isin(val_files)]
    test_data = data[data['SourceFile'].isin(test_files)]

    return train_data, val_data, test_data

# –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π (CNN)
def data_for_cnn_transmuter(train_data, val_data, test_data):
    train_dataset = nero.BatteryDatasetCNN(
        torch.tensor(train_data[features_cols].values, dtype=torch.float32).to(device),
        torch.tensor(train_data[target_variable].values, dtype=torch.float32).to(device),
        SEQUENCE_LENGTH,
        train_data['SourceFile'].values,
        train_data['Time [s]'].values
    )

    val_dataset = nero.BatteryDatasetCNN(
        torch.tensor(val_data[features_cols].values, dtype=torch.float32).to(device),
        torch.tensor(val_data[target_variable].values, dtype=torch.float32).to(device),
        SEQUENCE_LENGTH,
        val_data['SourceFile'].values,
        val_data['Time [s]'].values
    )

    test_dataset = nero.BatteryDatasetCNN(
        torch.tensor(test_data[features_cols].values, dtype=torch.float32).to(device),
        torch.tensor(test_data[target_variable].values, dtype=torch.float32).to(device),
        SEQUENCE_LENGTH,
        test_data['SourceFile'].values,
        test_data['Time [s]'].values
    )

    # —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –±–∞—Ç—á–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    return train_loader, val_loader, test_loader

# —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—â–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
def save_hyperparams(hyperparams, file_path):
    with open(file_path, 'w') as f:
        json.dump(hyperparams, f, indent=2)
    print(f"–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {file_path}")

# —á—Ç–µ–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞
def load_hyperparams(file_path):
    # —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
    with open(file_path, 'r') as f:
        hyperparams = json.load(f)
    return hyperparams

# –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –Ω–µ–º
def hyperparams_exist(file_path):
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
    if not os.path.exists(file_path):
        print(f"üìÅ –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return False

    hyperparams = load_hyperparams(file_path)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π
    required = ['hidden_size', 'num_layers', 'learning_rate']
    return all(key in hyperparams for key in required)

# –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
def train_education(model, criterion, optimizer, train_loader, val_loader, epochs, device, patience=20,
                       min_delta=0.001):
    # 1) –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    history = {'train_loss': [], 'val_loss': []}
    best_val_loss = float('inf')  # –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –ª—É—á—à—É—é validation loss
    epochs_no_improve = 0

    # 2) —ç–ø–æ—Ö–∞ - –æ–¥–∏–Ω –ø–æ–ª–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –≤—Å–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
    for epoch in range(epochs):
        # 2.1) –æ–±—É—á–∞–µ–º –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        model.train()
        train_loss = 0.0

        epoch_start_time = time.time() # (–Ω–∞—á–∞–ª–æ –æ—Ç—á–µ—Ç–∞) –∑–∞–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –ø–æ—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –Ω–∞ —ç–ø–æ—Ö—É
        for sequences, labels, _, _ in train_loader:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            sequences, labels = sequences.to(device), labels.to(device)
            labels = labels.unsqueeze(1)  # [128] ‚Üí [128, 1], —á—Ç–æ–± labels –∏ outputs –±—ã–ª–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã
            optimizer.zero_grad()  # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏  –∏ –∑–∞—á–µ–º - ?
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ (Forward Pass)
            outputs = model(sequences)  # –í—ã—Ö–æ–¥: [128, 1] - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ SOC
            loss = criterion(outputs, labels)  # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ (Backward Pass)
            loss.backward()
            optimizer.step() # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
            train_loss += loss.item() # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å

        epoch_end_time = time.time() # (–∫–æ–Ω–µ—Ü –æ—Ç—á–µ—Ç–∞) –∑–∞–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –ø–æ—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –Ω–∞ —ç–ø–æ—Ö—É
        epoch_time = epoch_end_time - epoch_start_time # –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —ç–ø–æ—Ö–∏

        train_loss = train_loss/len(train_loader) # –ø–æ–¥—Å—á–µ—Ç –æ–±—â–µ–π –æ—à–∏–±–∫–∏(–ø–æ –≤—Å–µ–º –±–∞—Ç—á–∞–º)
        history['train_loss'].append(train_loss)

        # 2.2) –ø—Ä–æ–≤–æ–¥–∏–º –≤–∞–ª–∏–¥–∞—Ü–∏—è; –º—ã —Ö–æ—Ç–∏–º –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –æ–¥–Ω–∏—Ö –∏ —Ç–µ—Ö –∂–µ –¥–∞–Ω–Ω—ã—Ö
        model.eval()  # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏
        val_loss = 0.0
        with torch.no_grad(): # —ç—Ç–æ "—Ä–µ–∂–∏–º —ç–∫–∑–∞–º–µ–Ω–∞" –¥–ª—è –º–æ–¥–µ–ª–∏, –≥–¥–µ –æ–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è, –Ω–æ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤—ã—Ö!
            for sequences, labels, _, _ in val_loader:
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                sequences, labels = sequences.to(device), labels.to(device)
                labels = labels.unsqueeze(1) # [128] ‚Üí [128, 1], —á—Ç–æ–± labels –∏ outputs –±—ã–ª–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã
                outputs = model(sequences) #  –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ SOC
                loss = criterion(outputs, labels) # –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
                val_loss += loss.item() # –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å

        val_loss /= len(val_loader) # –ø–æ–¥—Å—á–µ—Ç –æ–±—â–µ–π –æ—à–∏–±–∫–∏(–ø–æ –≤—Å–µ–º –±–∞—Ç—á–∞–º)
        history['val_loss'].append(val_loss)

        # 2.3) –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ; —Ä–∞–Ω–Ω—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
        if val_loss < best_val_loss - min_delta:
            best_val_loss = val_loss
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1

        print(f'–≠–ø–æ—Ö–∞ {epoch + 1}/{epochs}, –ü–æ—Ç–µ—Ä—è –æ–±—É—á–µ–Ω–∏—è: {train_loss}, –ü–æ—Ç–µ—Ä—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {val_loss}')
        print(f'–í—Ä–µ–º—è, –∑–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –Ω–∞ —ç–ø–æ—Ö—É: {epoch_time:.8f} —Å–µ–∫—É–Ω–¥')

        if epochs_no_improve >= patience: # 20 —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏–π
            print('–°—Ä–∞–±–æ—Ç–∞–ª–∞ –¥–æ—Å—Ä–æ—á–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞')
            break

    return history

# –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
def hyperparameters_selectioner(train_loader, val_loader):
    # 0) —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä—É—é –º—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º, –¥–∞—é—â–∞—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∑–∞–ø—É—Å–∫–∞—é—â–∞—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ä–∞—Å—á–∏—Ç—ã–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏
    def objective(trial):
        # 0.1) –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∑–∞–¥–∞–µ—Ç—Å—è –Ω–∏–∂–Ω–∏–π –∏ –≤–µ—Ä—Ö–Ω–º–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω)
        hidden_size = trial.suggest_categorical('hidden_size', [32, 64, 128, 264]) # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤ (feature maps) –≤ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—è—Ö
        num_layers = trial.suggest_int('num_layers', 1, 3) # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤ –≤ —Å–µ—Ç–∏
        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True) # c–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è - —à–∞–≥, —Å –∫–æ—Ç–æ—Ä—ã–º –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –≤–µ—Å–∞ —Å–µ—Ç–∏
        kernel_size = trial.suggest_categorical('kernel_size', [3]) # —Ä–∞–∑–º–µ—Ä —è–¥—Ä–∞ —Å–≤–µ—Ä—Ç–∫–∏
        dropout = trial.suggest_float('dropout', 0.1, 0.5) # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è

        # 0.2) —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        model = nero.SoCCNN(input_size=len(features_cols), hidden_size=hidden_size, num_layers=num_layers,
                            kernel_size=kernel_size, dropout=dropout).type(torch.float32).to(device)

        # 0.3) –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–≤–æ—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)

        # 0.4) –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å [–º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∞, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏–µ –¥–∞—Ç—ã, —ç–ø–æ—Ö–∏, device]
        history = train_education(model, criterion, optimizer, train_loader, val_loader, EPOCHS, device)

        # 0.5) –ò–∑–≤–ª–µ—á—å –ø–æ—Å–ª–µ–¥–Ω—é—é –ø–æ—Ç–µ—Ä—é –ø—Ä–æ–≤–µ—Ä–∫–∏
        last_val_loss = history['val_loss'][-1]
        return last_val_loss

    # 1) –∑–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç—É–Ω—É –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞, –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=15)

    # 2) –∏–∑–≤–ª–µ–∫–∞–µ–º –ª—É—á—à—É—é –≤–µ—Ä—Å–∏—é
    best_trial = study.best_trial
    print(f"Best trial: {best_trial}")
    best_hyperparams = study.best_trial.params
    print('Best CNN hyperparameters:', best_hyperparams)

    # 3) –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    optimization_history = plot_optimization_history(study)
    optimization_history.show()
    # –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    param_importances = plot_param_importances(study)
    param_importances.show()
    return best_trial, best_hyperparams

# —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ-–∏—Ç–æ–≥–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
def finale_model_trainer(best_hyperparams, train_loader, val_loader, model_path):
    # 1) –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∑–∞–¥–∞–µ—Ç—Å—è –Ω–∏–∂–Ω–∏–π –∏ –≤–µ—Ä—Ö–Ω–º–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω)
    hidden_size = best_hyperparams['hidden_size']
    num_layers = best_hyperparams['num_layers']
    learning_rate = best_hyperparams['learning_rate']
    kernel_size = best_hyperparams.get('kernel_size', 3)
    dropout = best_hyperparams.get('dropout', 0.2)
    epochs = 20

    # 2) —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    model = nero.SoCCNN(input_size=len(features_cols), hidden_size=hidden_size, num_layers=num_layers,
                        kernel_size=kernel_size, dropout=dropout).type(torch.float32).to(device)

    # 3) –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ —Å–≤–æ—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)

    # 4) –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å [–º–æ–¥–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∞, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏–µ –¥–∞—Ç—ã, —ç–ø–æ—Ö–∏, device]
    history = train_education(model, criterion, optimizer, train_loader, val_loader, epochs, device)

    # 5) –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    torch.save({'model_state_dict': model.state_dict(), 'input_size': len(features_cols)}, model_path)

# —Ç–µ—Å—Ç–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏
def finale_model_tester(best_hyperparams, test_loader, model_path):
    # 1) –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∑–∞–¥–∞–µ—Ç—Å—è –Ω–∏–∂–Ω–∏–π –∏ –≤–µ—Ä—Ö–Ω–º–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω)
    hidden_size = best_hyperparams['hidden_size']
    num_layers = best_hyperparams['num_layers']
    kernel_size = best_hyperparams.get('kernel_size', 3)
    dropout = best_hyperparams.get('dropout', 0.2)

    # 1.1) –∑–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    loaded_model = nero.SoCCNN(input_size=len(features_cols), hidden_size=hidden_size, num_layers=num_layers,
                               kernel_size=kernel_size, dropout=dropout).type(torch.float32).to(device)
    loaded_model.load_state_dict(torch.load(model_path, map_location=device)['model_state_dict'])
    loaded_model.to(device)
    loaded_model.eval()

    # 1.2) –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    test_predictions = []
    test_labels = []

    with torch.no_grad():
        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ - –ø–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        for sequences, labels, _, _ in test_loader:
            sequences, labels = sequences.to(device), labels.to(device)
            outputs = loaded_model(sequences)
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            test_predictions.extend(outputs.cpu().view(-1).tolist())
            test_labels.extend(labels.cpu().view(-1).tolist())

    # 1.3) —Ä–∞—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã –∏ –º–µ—Ç–∫–∏ –≤ –º–∞—Å—Å–∏–≤—ã numpy –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –æ—à–∏–±–æ–∫
    test_predictions_np = np.array(test_predictions)
    test_labels_np = np.array(test_labels)

    # MSE –∏ MAE
    mse = mean_squared_error(test_labels_np, test_predictions_np)
    mae = mean_absolute_error(test_labels_np, test_predictions_np)

    print(f"CNN Mean Squared Error on Test Set: {mse:.6f}")
    print(f"CNN Mean Absolute Error on Test Set: {mae:.6f}")

def main(data_directory_dict, model_path, hyperparams_path):
    # 1) –∑–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ, –∞ —Ç–∞–∫–∂–µ —Å–æ–≤–µ—Ä—à–∞–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É
    directory = data_directory_dict["LG_HG2_processed"]
    # temperatures_directory = [folder for folder in os.listdir(directory) if 'degC' in folder]
    temperatures_directory = [folder for folder in os.listdir(directory) if 'degC' in folder]
    data = data_loader_and_standarder(temperatures_directory, directory)
    # 1.1) —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Ç–µ—Å—Ç–æ–≤—É—é –∏ –æ–±—É—á–∞—é—â–∏–µ –≤—ã–±–æ—Ä–∫–∏
    percents = [0.2, 0.5]
    train_data, val_data, test_data = data_spliter(data, percents)
    # 1.2) –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —á—Ç–µ–Ω–∏—è –∏—Ö –º–æ–¥–µ–ª—å—é, —Ç–∞–∫ –∫–∞–∫ —Ç—Ä–µ–±—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã.
    train_loader, val_loader, test_loader = data_for_cnn_transmuter(train_data, val_data, test_data)

    if not hyperparams_exist(hyperparams_path):
        # 2) –ø–æ–¥–±–∏—Ä–∞–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        print("–ø–æ–¥–±–µ—Ä–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
        best_trial, best_hyperparams = hyperparameters_selectioner(train_loader, val_loader)
        # 2.1) —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        save_hyperparams(best_hyperparams, hyperparams_path)
    else:
        # 3) —Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        print("—Ä–µ–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–≥–Ω–æ–∑")
        best_hyperparams = load_hyperparams(hyperparams_path)
        finale_model_trainer(best_hyperparams, train_loader, val_loader, model_path)
        # 4) —Ç–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        finale_model_tester(best_hyperparams, test_loader, model_path)

    return

if __name__ == "__main__":
    model_path = "/Users/nierra/Desktop/–¥–∏–ø–ª–æ–º-2/–¥–∞—Ç–∞—Å–µ—Ç_2/soc_lstm_model.pth"
    cnn_hyperparams_path = "/Users/nierra/Desktop/–¥–∏–ø–ª–æ–º-2/–¥–∞—Ç–∞—Å–µ—Ç_2/cnn_best_hyperparams.json"
    main_directory = "/Users/nierra/Desktop/–¥–∏–ø–ª–æ–º-2/–¥–∞—Ç–∞—Å–µ—Ç_2/Data"
    data_directory_dict = {"LG_HG2_processed": f"{main_directory}/LG_HG2_processed"}
    main(data_directory_dict, model_path, cnn_hyperparams_path)